
\subsection*{Software} 

  We are pleased to make available two software repositories accompanying this analysis. 
  \begin{itemize}
    \item Package 
    \texttt{compx} for the \texttt{R} programming language implements computation of the information measures $H(Y)$, $I(X,Y)$, and $J(X,Y)$, as well as a method for information-theoretic clustering. Access \texttt{compx} at 
    \begin{displayquote}
      \texttt{https://github.com/PhilChodrow/compx}. 
    \end{displayquote}
    To install in \texttt{R}, install the package \texttt{devtools} at 
    \begin{displayquote}
    \texttt{https://cran.r-project.org/web/packages/devtools/index.html}
    \end{displayquote}
     and run the command 
    \begin{displayquote}
      \texttt{devtools::install\_github(``PhilChodrow/compx'')}
    \end{displayquote}
    in the \texttt{R} console. 
    \item The analysis repository for this project, including data acquisition and processing; core computations; and figure generation. We hope that this repository will provide useful examples of how to use \texttt{compx} for others aiming to replicate and extend our results. Download the project files at 
    \begin{displayquote}
      \texttt{https://github.com/PhilChodrow/spatial\_complexity}. 
    \end{displayquote}
  \end{itemize}

\subsection*{Relation of mutual and Fisher informations}
	Let $X$ be a continuous random variable taking values in $\R^n$, and let $Y$ be a discrete random variable define on finite alphabet $\mathcal{Y}$. Suppose further that $p(y|x) > 0$ and that $p(y|x)$ is differentiable as a function of $x$ for all $x,y \in \R^n \times \mathcal{Y}$. Fix $x_0 \in \R^n$, and define $B_r \triangleq B_r(x_0) = \{ x \in R^n \;|\; \norm{x - x_0} \leq r \}$. Additionally, define the \emph{local mutual information} in $B_r$ as the mutual information between $X$ and $Y$ where $X$ is restricted to $B_r$:
	\begin{align}
		I_r(x_0) &\triangleq \E_X[D[p(\cdot|X)\| p(\cdot|X \in B_r)]|X \in B_r] \\
		&= \int_{B_r} p(x|X \in B_r) D[p(\cdot|x)\| p(\cdot|X \in B_r)] d^n x\;.
	\end{align}
	where $D[p\|q] \triangleq \sum_{y} p(y) \log \frac{p(y)}{q(y)}$ is the Kullback-Leibler divergence of $q$ from $p$. 

	\begin{thm} \label{thm1}
		Under the stated conditions, 
		\begin{equation}
			\lim_{r\rightarrow 0} \frac{I_r(x_0)}{r^2} = \frac{n}{2(n+2)} \emph{trace}\; J_Y(x_0)\;.
		\end{equation}
		where the Fisher information matrix $J_Y$ is given by 
		\begin{align}
		J_Y(x) &\triangleq \E_Y\left[\nabla_x S_{Y}(x)\nabla_x S_{Y}(x)^T \right] \\
		S_y(x) &\triangleq \log p(y|x)\;.
	\end{align}
	\end{thm}

	The proof of Theorem \ref{thm1} proceeds by the application of a number of Taylor approximations, in tandem with a fundamental relationship of information geometry. We first expand out $I_r(x_0)$ explicitly as
	\begin{equation}
		I_r(x_0) = \int_{B_r} p(x|X \in B_r) D[p(\cdot|x)\| p(\cdot|X \in B_r)] d^n x\;. \label{eq:explicit}
	\end{equation}

	\begin{lm} \label{approximations}
		The following approximation relationships hold for the components of \eqref{eq:explicit}:
		\begin{enumerate}[label=\emph{(\alph*)}]
			\item $p(X\in B_r,Y) = p(x_0, Y) v(B_r) + O(r^{n+2})$
			\item $p(Y|X \in B_r) = p(Y|x_0) + e_y$ where the error terms $e_y$ satisfy $e_y \in O(r^{2})$ and $\sum_{y \in \mathcal{Y}} e_y = 0$. 
			\item $p(x|X \in B_r) = \frac{1 + O(r)}{v(B_r)}$
		\end{enumerate}
	\end{lm}
	\begin{proof}
		For each approximation,  we directly apply Taylor expansions about $X = x_0$. 
		\begin{enumerate}[label=(\alph*)]
			\item We have 
				\begin{align}
					p(X\in B_r, Y) &= \int_{B_r} p(x,Y) \; d^nx \\
					&= \int_{B_r} p(x_0,Y) + \frac{\partial p(x_0, Y)}{\partial x}(x - x_0) + O(\norm{x - x_0}^2) \; d^nx \\
					&= p(x_0, Y)v(B_r) +  \frac{\partial p(x_0, Y)}{\partial x} \int_{B_r} (x - x_0)\; d^nx  \\
					&\quad \quad+ O\left(\int_{B_r} \norm{x - x_0}^2\; d^nx\right) \\
					&= p(x_0, Y)v(B_r) + O(r^{n+2})\;,
				\end{align}
				where the middle term vanishes due to spherical symmetry. 
			\item The fact that the error terms $e_y$ must satisfy $\sum_{y \in \mathcal{Y}} e_y = 0$ follows from the fact that $p(Y|X \in B_r)$ must be a valid probability distribution over $\mathcal{Y}$. We'll now show that $e_y \in O(r^{ÃŸ2})$. First, 
				\begin{align}
					p(X \in B_r) &= \sum_{y\in \mathcal{Y}} p(X \in B_r, y)  \\
					&= \sum_{y\in \mathcal{Y}} \left[p(x_0, y)v(B_r) + O(r^{n+2}) \right] \\
					&= p(x_0)v(B_r) + O(r^{2});\,
				\end{align}
			from part (a). Next, 
				\begin{align}
					p(Y|X \in B_r) &= \frac{p(X\in B_r, Y)}{p(X \in B_r)} \\
					&= \frac{p(x_0, Y)v(B_r) + O(r^{n+2})}{p(x_0)v(B_r) + O(r^{n+2})} \\
					&= p(Y|x_0) + O(r^2)\;,
				\end{align}
			which completes this part of the argument. 
			\item First, 
				\begin{align}
					p(X \in B_r) &= \int_{B_r} p(x) \; d^nx  \\
					&= \int_{B_r} \left[p(x_0) + \nabla p(x_0)(x - x_0) + O(r^2)\right] \; d^nx \\ 
					&= p(x_0) v(B_r) + O(r^{n+2})\;,
				\end{align}
				where the middle term again vanishes through spherical symmetry. Thus, for $x \in B_r$, we have
				\begin{align}
					p(x | X \in B_r) &= \frac{p(x)}{p(X \in B_r)} \\
					&= \frac{p(x_0) + \nabla p(x_0)(x - x_0) + O(r^2)}{p(x_0) v(B_r) + O(r^{n+2})} \\
					&= \frac{1 + O(r)}{v(B_r)}\;.
				\end{align}
		\end{enumerate}
	\end{proof}

	\begin{lm} The following approximation holds for the divergence factor in the integral \eqref{eq:explicit}
		\begin{equation}
			D[p(\cdot|x)\| p(\cdot|X \in B_r)] = D[p(\cdot|x)\| p(\cdot|x_0)] + O(r^{3})
		\end{equation}
	\end{lm}
	\begin{proof}
		We compute directly: 
		\begin{align}
			D[p(\cdot|x)\| p(\cdot|X \in B_r)] &= \sum_{y \in \mathcal{Y}} p(y|x) \log \frac{p(y|x)}{p(y|X \in B_r)} \\
			&= - H[Y|X = x] - \sum_{y \in \mathcal{Y}} p(y|x) \log p(y|X \in B_r) \\
			&= - H[Y|X = x] - \sum_{y \in \mathcal{Y}} p(y|x) \log \left(p(y|x_0) + e_y)\right) \tag{from Lemma \ref{approximations}}\\
			&= - H[Y|X = x] - \sum_{y \in \mathcal{Y}} p(y|x) \left[\log p(y|x_0) + \frac{e_y}{p(y|x_0)} + O(e_y^2)\right] \\
			&= D[p(\cdot|x)\| p(\cdot|x_0)] + \sum_{y \in \mathcal{Y}} \frac{p(y|x)}{p(y|x_0)}  e_y \tag{quadratic terms negligible} \\
			&=D[p(\cdot|x)\| p(\cdot|x_0)] \\
			&\quad + \sum_{y \in \mathcal{Y}} \left(1 + \frac{1}{p(y|x_0)} \nabla p(y|x_0)(x - x_0) + O(r^{2})\right)    e_y \\
			&= D[p(\cdot|x)\| p(\cdot|x_0)] + \sum_{y \in \mathcal{Y}} \left[e_y + O(r^{3})\right] \tag{$e_y \in O(r^2)$}\\
			&= D[p(\cdot|x)\| p(\cdot|x_0)] + O(r^3) \tag{$\sum_{y \in \mathcal{Y}} e_y = 0$}
		\end{align}
	\end{proof}

	\begin{lm} For any positive-semidefinite matrix $A \in \R^{n\times n}$, 
		$$\int_{B_r} \left< x - x_0, A(x-x_0) \right> d^n x = \frac{n}{n+2} r^{2} v(B_r) \text{\emph{trace}} (A)$$
	\end{lm}
		
	\begin{proof}
		Since $A$ is positive-semidefinite, there exist an orthonormal matrix $P$ and a diagonal matrix $D$ such that $A = P^TDP$. Furthermore, the entries of $D$ are the eigenvalues $\{\lambda_i\}$ of $A$. Then, 
		\begin{align}
			\int_{B_r} \left< x - x_0, A(x-x_0) \right> d^n x &= \int_{B_r} \left< x - x_0, P^T DP(x-x_0) \right> d^n x\\
			&= \int_{B_r} \left< P(x - x_0),  DP(x-x_0) \right> d^n x \;.
		\end{align}
		We can regard $P$ as a reparameterization of $B_r$; since $\det P = 1$, we have 
		\begin{align}
			\int_{B_r} \left< P(x - x_0),  DP(x-x_0) \right> d^n x &= \int_{B_r} \left< x - x_0,  D(x-x_0) \right> d^n x \\
			&= r^n\int_{B_n} \left< rx, rDx \right> d^n x \\
			&= r^{n+2}\int_{B_n} \left< x, Dx \right> d^n x \;,
		\end{align}
		where $B_n$ is the unit $n$-ball. We also let $S_n(r)$ be the $n$-sphere of radius $r$. Continuing, 
		\begin{align}
			r^{n+2}\int_{B_n} \left< x, Dx \right> d^n x &= r^{n+2} \int_{B_n} \sum_{i= 1}^n x_i^2 \lambda_i \; d^nx \\
			&= r^{n+2} \sum_{i = 1}^n \lambda _i \int_{B_n} x_i^2 \; d^nx \\
			&= \frac{r^{n+2}}{n} \sum_{i = 1}^n \lambda _i \int_{B_n} \norm{x}^2 \; d^nx \\ \tag{spherical symmetry} \\
			&= \frac{r^{n+2}}{n} \text{trace}(A)  \int_{B_n} \norm{x}^2 \; d^nx \\ \tag{spherical symmetry} \\
			&= \frac{r^{n+2}}{n} \text{trace} (A) \int_{\rho \in [0,1]} \rho^2 S_{n-1}(\rho) d\rho \\
			&= \frac{r^{n+2}}{n} \text{trace} (A) \int_{\rho \in [0,1]} \rho^{n+1} S_{n-1}(1) d\rho \\
			&= \frac{r^{n+2}}{n} \text{trace} (A)  \frac{1}{n+2} S_{n-1}(1) \\
			&= \frac{r^2}{n+2}  \text{trace} (A)  n r^{n}v(B_n(1)) \\
			&= \frac{n}{n+2} r^{2} v(B_r) \text{trace} (A)\;,
		\end{align}
		as was to be shown. 
	\end{proof}

	\begin{fct*}
		The Kullback-Leibler divergence and the Fisher information $J_Y$ are related according to the approximation 
		\begin{equation}
			D[p(\cdot|x)\| p(\cdot|x_0)] = \frac{1}{2}\left<x - x_0, J_Y(x_0)(x - x_0) \right> + O(\norm{x - x_0}^3)
		\end{equation}
	\end{fct*}

	We are finally ready to prove Theorem \ref{thm1}. Computing directly, we have 
	\begin{align}
		I_r(x_0) &\triangleq \E_X[D[p(\cdot|X)\| p(\cdot|X \in B_r)]|X \in B_r]\;. \\
		&= \int_{B_r} p(x|X \in B_r) D[p(\cdot|x)\| p(\cdot|X \in B_r)] d^n x \\
		&= \int_{B_r} \left[\frac{1 + O(r)}{v(B_r)}\right] D[p(\cdot|x)\| p(\cdot|X \in B_r)] d^n x \tag{Lemma 1(c)}\\ 
		&= \left[\frac{1 + O(r)}{v(B_r)}\right]\int_{B_r}  \left(D[p(\cdot|x)\| p(\cdot|x_0)] + O(r^{3})\right) d^n x \tag{Lemma 2}\\
		&= \left[\frac{1 + O(r)}{v(B_r)}\right]\int_{B_r} \left( \frac{1}{2}\left<x - x_0, J_Y(x_0)(x - x_0) \right> + O(\norm{x - x_0}^3) + O(r^{3})\right) d^n x \\
		&= \frac{1}{2}\left[\frac{1 + O(r)}{v(B_r)}\right]\int_{B_r} \left(\left<x - x_0, J_Y(x_0)(x - x_0) \right> + O(r^{3})\right) d^n x \\
		&= \frac{1}{2}\left[\frac{1 + O(r)}{v(B_r)}\right]\left(\frac{n}{n+2} r^{2} v(B_r) \text{trace} (J_Y(x_0)) + v(B_r)O(r^{3})\right) \\
		&= r^2\frac{n}{2(n+2)}\left[1 + O(r)\right]\left(\text{trace} (J_Y(x_0)) + O(r^{3})\right)\;.
	\end{align}
	Dividing through by $r^2$ and computing the limit as $r \rightarrow 0$ proves the result. 


\subsection*{Computational Methods and Assumptions}
	In this section, we provide a specification of the computational procedure used to estimate $J(X,Y) = \E_x[J_Y(X)]$ using blockgroup level data from the U.S. Census. 

	For fixed Census blockgroup $i$, let $P_i$ be the population, let $A_i$ be the area, let $\rho_i = P_i / A_i$ be the population density, and let $p^i_Y(y)$ be the observed proportion of racial group $y$. For hex $k$ in our hexagonal grid, let $N_k$ be the set of overlapping Census blockgroups. We also define $p^{k}_I(i) = \rho_i / \sum_{i \in N_k} \rho_i$ as the estimated proportion of population within hex $k$ residing in blockgroup $i$. This definition embodies a computationally-simplifying assumption that each blockgroup in $N_k$ overlaps hex $k$ with equal area. Finally, $p^k_Y(y) = \sum_{i \in N_k} p^{k}_I(i) p^i_Y(y)$ is the estimated overall racial composition of hex $k$. Then, we estimate the mutual information in hex $k$ as 
	\begin{equation}
		I(k) = \sum_{i \in N_k} p^k_I(i) D[p^i_Y(\cdot) \| p^k_Y(\cdot)]\;. 
	\end{equation}
	Using \eqref{eq:approx}, the estimated Fisher information is 
	\begin{equation}
		J(k) \approx \frac{4 I(k)}{r^2}
	\end{equation}
	where $r$ is the grid radius. The estimated population in hex $k$ is $P_k = A_k\sum_{i \in N_k} \rho_i$, where $A_k$ is the cell area. We finally estimate $\E_X[J(X)]$ as 
	\begin{equation}
		J(X,Y) = \E_X[J_Y(X)] \approx \frac{1}{\sum_k P_k} \sum_k P_k J(k)
	\end{equation}


\subsection{Specification of Spatially-Constrained Information-Theoretic Clustering}
