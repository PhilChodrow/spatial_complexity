
\subsection*{Software} 

  We are pleased to make available two software repositories accompanying this analysis. 
  \begin{itemize}
    \item Package 
    \texttt{compx} for the \texttt{R} programming language implements computation of the information measures $H(Y)$, $I(X,Y)$, and $J(X,Y)$, as well as a method for information-theoretic clustering. Access \texttt{compx} at 
    \begin{displayquote}
      \texttt{https://github.com/PhilChodrow/compx}. 
    \end{displayquote}
    To install in \texttt{R}, install the package \texttt{devtools} at 
    \begin{displayquote}
    \texttt{https://cran.r-project.org/web/packages/devtools/index.html}
    \end{displayquote}
     and run the command 
    \begin{displayquote}
      \texttt{devtools::install\_github(``PhilChodrow/compx'')}
    \end{displayquote}
    in the \texttt{R} console. 
    \item The analysis repository for this project, including data acquisition and processing; core computations; and figure generation. We hope that this repository will provide useful examples of how to use \texttt{compx} for others aiming to replicate and extend our results. Download the project files at 
    \begin{displayquote}
      \texttt{https://github.com/PhilChodrow/spatial\_complexity}. 
    \end{displayquote}
  \end{itemize}

\subsection*{Relationship of Mutual and Fisher Informations}

	Let $X$ be a continuous random variable taking values in $\R^n$, and let $Y$ be a discrete random variable define on finite alphabet $\mathcal{Y}$. Suppose further that $p(y|x) > 0$ and that $p(y|x)$ is differentiable as a function of $x$ for all $x,y \in \R^n \times \mathcal{Y}$. Fix $x_0 \in \R^n$, and define $B_r \triangleq B_r(x_0) = \{ x \in R^n \;|\; \norm{x - x_0} \leq r \}$. Additionally, define the \emph{local mutual information} in $B_r$ as the mutual information between $X$ and $Y$ where $X$ is restricted to $B_r$:
		\begin{align}
			I_r(x_0) &\triangleq \E_X[D[p(\cdot|X)\| p(\cdot|X \in B_r)]|X \in B_r] \\
			&= \int_{B_r} p(x|X \in B_r) D[p(\cdot|x)\| p(\cdot|X \in B_r)] d^n x\;.
		\end{align}
		where $D[p\|q] \triangleq \sum_{y} p(y) \log \frac{p(y)}{q(y)}$ is the Kullback-Leibler divergence of $q$ from $p$. 

		\begin{thm} \label{thm1}
			Under the stated conditions, 
			\begin{equation}
				\lim_{r\rightarrow 0} \frac{I_r(x_0)}{r^2} = \frac{n}{2(n+2)} \emph{trace}\; J_Y(x_0)\;.
			\end{equation}
			where the Fisher information matrix $J_Y$ is given by 
			\begin{align}
			J_Y(x) &\triangleq \E_Y\left[\nabla_x S_{Y}(x)\nabla_x S_{Y}(x)^T \right] \\
			S_y(x) &\triangleq \log p(y|x)\;.
		\end{align}
		\end{thm}

	We first define the following functions. For set $A$, let 
	\begin{equation}
		f_A(x) \triangleq  D[p(y|x) \| p(y|x\in A)]Â \;.
	\end{equation}
	Additionally, let 
	\begin{align}
		q_r(x) &\triangleq p(x|X \in B_r)\;.
	\end{align}
	We can then write the local mutual information as 
	\begin{equation}
		I_r(x_0) = \int_{B_r} q_r f_{B_r} \; d\lambda\;,
	\end{equation}
	where $\lambda$ is the Lebesgue measure in $\R^n$. We also let $V_r \triangleq \int_{B_r} \; d\lambda$ be the volume of $B_r$. Finally, define 
	\begin{equation}
		a_r(y) \triangleq p(X \in B_r, Y). 
	\end{equation}

	\begin{lm}
		For all $x \in B_r$,  
		\begin{equation}
			q_r(x) = \frac{1 + O(r)}{V_r}\;.	
		\end{equation}
	\end{lm}
	\begin{proof}
		First, Taylor's theorem implies that there exists a linear map $T$ such that 
		\begin{align}
			p(x) &= p(x_0) + T(x - x_0) + O(\norm{x - x_0}^2) \\
			&= p(x_0) + T(x - x_0) + O(r^2)\;. 
		\end{align}
		 Then, 
		\begin{align}
			p(X\in B_r) &= \int_{B_r} p \; d\lambda \\
			&= \int_{B_r} p(x_0) + T(x - x_0) + O(r^2) d^n x\;.
		\end{align}
		The middle term vanishes by spherical symmetry, leaving 
		\begin{align}
			p(X\in B_r) &= p(x_0)V_r  + O(r^{n+2})\;.
		\end{align}
		For $x \in B_r$, we therefore have 
		\begin{align}
			p(x|X \in B_r) &= \frac{p(x,X \in B_r)}{p(X \in B_r)} \\ 
			&= \frac{p(x)}{p(X \in B_r)} \\
			&= \frac{p(x_0) + T(x - x_0) + O(r^2)}{p(x_0)V_r  + O(r^{n+2})} \\
			&= \frac{p(x_0) + O(r) + O(r^2)}{p(x_0)V_r} \\
			&= \frac{1 + O(r)}{V_r}\;,
		\end{align}
		as was to be shown. 
	\end{proof}

	\begin{lm}
		For all $y\in Y$,
		\begin{equation}
			a_r(y) = p(x_0,y)V_r + O(r^{n+2})\;.
		\end{equation}
	\end{lm}
	\begin{proof}
		Taylor's Theorem again implies that, for any $(x,y)\in B_r \times \mathcal{Y}$, there is a linear map $T$ such that
		\begin{align}
			p(x,y) &= p(x_0,y) + T(x - x_0) + O(\norm{x - x_0}^2) \\
			&= p(x_0,y) + T(x - x_0) + O(r^2). 
		\end{align}
		We then have 
		\begin{align}
			p(X \in B_r, y) &= \int_{B_r} p(x,y) \; d^n x \\ 
			&= \int_{B_r} p(x_0,y) + T(x - x_0) + O(r^2) \; d^n x \\
			&= p(x_0,y)V_r + O(r^{n+2}) \; d^n x\;,
		\end{align}
		where the middle term has again vanished due to spherical symmetry. 
	\end{proof}
	\begin{lm}
		For any $y \in \mathcal{Y}$, we have $p(y|X \in B_r) = p(y|x_0) + e_y$, where the error terms $e_y$ satisfy $e_y \in O(r^2)$ and $\sum_{y \in \mathcal{Y}} e_y = 0$. 
	\end{lm}
	\begin{proof}
		That the errors must satisfy $\sum_{y \in \mathcal{Y}} e_y = 0$ follows from the fact that $p(\cdot| X \in B_r)$ must be a valid probability distribution on $\mathcal{Y}$. We'll now show that $e_y \in O(r^2)$. We then have 
		\begin{align}
			p(y|X \in B_r) &= \frac{p(X \in B_r, Y)}{p(X\in B_r)} \\
			&= \frac{p(x_0, y)V_r + O(r^{n+2})}{p(x_0)V_r + O(r^{n+2})} \\
			&= p(y|x_0)  O(r^{2}) \tag{$V_r \propto r^n$}\;,
		\end{align}
		as needed. 
	\end{proof}
	\begin{lm}
		For any $x \in B_r$, 
		\begin{equation}
			f_{B_r}(x) = f_{x_0}(x) + O(r^3)\;.
		\end{equation}
	\end{lm}
	\begin{proof}
		We compute directly: 
		\begin{align}
			f_{B_r}(x) &= D[p(\cdot|x)\|p(\cdot|X \in B_r)] \\
			&= \sum_{y\in \mathcal{Y}} p(y|x) \log \frac{p(y|x)}{p(y|X \in B_r)} \\
			&= \sum_{y\in \mathcal{Y}} p(y|x) \log p(y|x) - \sum_{y \in \mathcal{Y}} p(y|x) \log p(y|X \in B_r)\; \\
			&= \sum_{y\in \mathcal{Y}} p(y|x) \log p(y|x) - \sum_{y \in \mathcal{Y}} p(y|x) \log (p(y|x_0) + e_y) \\ 
			&= \sum_{y\in \mathcal{Y}} p(y|x) \log p(y|x) - \sum_{y \in \mathcal{Y}} p(y|x) \log p(y|x_0) + \frac{e_y}{p(y|x_0)} + O(e_y^2)   \\
			&= \sum_{y\in \mathcal{Y}} p(y|x) \log p(y|x) - \sum_{y \in \mathcal{Y}} p(y|x) \log p(y|x_0) + \sum_{y\in \mathcal{Y}} \frac{p(y|x)}{p(y|x_0)}e_y + O(e_y^2) \\
			&= D[p(\cdot|x)\|p(\cdot|x_0)] + \sum_{y\in \mathcal{Y}} \frac{p(y|x)}{p(y|x_0)}e_y + O(e_y^2) \\
			&= f_{x_0}(x) + \sum_{y\in \mathcal{Y}} \frac{p(y|x)}{p(y|x_0)}e_y + O(e_y^2)\;.
		\end{align}
		It remains to show that the error term is of order $O(r^3)$. Invoking Taylor's Theorem again, there exists a linear map $T$ such that 
		\begin{align}
			\sum_{y\in \mathcal{Y}} \frac{p(y|x)}{p(y|x_0)}e_y + O(e_y^2) &= \sum_{y\in \mathcal{Y}} \left(1 +  \frac{1}{p(y|x_0)} T(x - x_0) + O(\norm{x - x_0}^2) \right)e_y + O(e_y^2) \\
			&= \sum_{y\in \mathcal{Y}} \left(1 +  O(r) + O(r^2) \right)e_y + O(e_y^2) \\
			&= \sum_{y\in\mathcal{Y}}\left[e_y + O(r^3)\right] \\
			&= O(r^3)\;, 
		\end{align}
		since $\sum_{y\in \mathcal{Y}} e_y = 0$ by the previous lemma. 
	\end{proof}

	We now invoke the following well-known theorem of information geometry; for reference see \cite{Amari2000}. 

	\begin{thm}
		The Kullback-Leibler divergence and Fisher information $J_Y$ are related according to the following approximation: 
		\begin{equation}
			D[p(\cdot|x)\|p(\cdot|x_0)] = \frac{1}{2} \left<x - x_0, J_Y(x_0)(x - x_0)\right> + O(\norm{x - x_0}^3)\;. 
		\end{equation}
	\end{thm}

	Theorem 2 allows us to write $I_r(x_0)$ as the following spherical integral: 

	\begin{align}
		I_r(x_0) &= \int_{B_r} q_r f_{B_r} \; d\lambda \\ 
		&= \int_{B_r} q_r(x) f_{B_r}(x_0)\; d^n x \\
		&= \int_{B_r} \frac{1 + O(r)}{V_r} \left(f_{x_0}(x) + O(r^3)\right) \; d^n x \\
		&= \frac{1 + O(r)}{V_r} \int_{B_r} \left(f_{x_0}(x) + O(r^3)\right) \; d^n x \\
		&= \frac{1}{2}\frac{1 + O(r)}{V_r} \int_{B_r} \left( \left<x - x_0, J_Y(x_0)(x - x_0)\right>  + O(r^3)\right) \; d^n x \\ 
		&= \frac{1}{2}\frac{1 + O(r)}{V_r} \left(\int_{B_r}  \left<x - x_0, J_Y(x_0)(x - x_0)\right> d^n x + V_rO(r^3)\right). 
	\end{align}
	It remains to compute the integral, which we do using the following lemma: 

	\begin{lm} For any positive-semidefinite matrix $A \in \R^{n\times n}$, 
			$$\int_{B_r} \left< x - x_0, A(x-x_0) \right> d^n x = \frac{n}{n+2} r^{2} V_r \text{\emph{trace}} (A)$$
	\end{lm}
		
	\begin{proof}
		Since $A$ is positive-semidefinite, there exist an orthonormal matrix $P$ and a diagonal matrix $D$ such that $A = P^TDP$. Furthermore, the entries of $D$ are the eigenvalues $\{\lambda_i\}$ of $A$. Then, 
		\begin{align}
			\int_{B_r} \left< x - x_0, A(x-x_0) \right> d^n x &= \int_{B_r} \left< x - x_0, P^T DP(x-x_0) \right> d^n x\\
			&= \int_{B_r} \left< P(x - x_0),  DP(x-x_0) \right> d^n x \;.
		\end{align}
		We can regard $P$ as a reparameterization of $B_r$; since $\det P = 1$, we have 
		\begin{align}
			\int_{B_r} \left< P(x - x_0),  DP(x-x_0) \right> d^n x &= \int_{B_r} \left< x - x_0,  D(x-x_0) \right> d^n x \\
			&= r^n\int_{B_n} \left< rx, rDx \right> d^n x \\
			&= r^{n+2}\int_{B_n} \left< x, Dx \right> d^n x \;,
		\end{align}
		where $B_n$ is the unit $n$-ball. We also let $S_n(r)$ be the $n$-sphere of radius $r$. Continuing, 
		\begin{align}
			r^{n+2}\int_{B_n} \left< x, Dx \right> d^n x &= r^{n+2} \int_{B_n} \sum_{i= 1}^n x_i^2 \lambda_i \; d^nx \\
			&= r^{n+2} \sum_{i = 1}^n \lambda _i \int_{B_n} x_i^2 \; d^nx \\
			&= \frac{r^{n+2}}{n} \sum_{i = 1}^n \lambda _i \int_{B_n} \norm{x}^2 \; d^nx \tag{spherical symmetry} \\
			&= \frac{r^{n+2}}{n} \text{trace}(A)  \int_{B_n} \norm{x}^2 \; d^nx \\ 
			&= \frac{r^{n+2}}{n} \text{trace} (A) \int_{\rho \in [0,1]} \rho^2 S_{n-1}(\rho) d\rho \\
			&= \frac{r^{n+2}}{n} \text{trace} (A) \int_{\rho \in [0,1]} \rho^{n+1} S_{n-1}(1) d\rho \\
			&= \frac{r^{n+2}}{n} \text{trace} (A)  \frac{1}{n+2} S_{n-1}(1) \\
			&= \frac{r^2}{n+2}  \text{trace} (A)  n r^{n}v(B_n(1)) \\
			&= \frac{n}{n+2} r^{2} V_r \text{trace} (A)\;,
		\end{align}
		as was to be shown. 
	\end{proof}

	We may therefore complete the proof: 

	\begin{align}
		I_r(x_0) &= \frac{1}{2}\frac{1 + O(r)}{V_r} \left(\int_{B_r}  \left<x - x_0, J_Y(x_0)(x - x_0)\right> d^n x + V_rO(r^3)\right) \\ 
		&= \frac{1}{2}\frac{1 + O(r)}{V_r} \left[\frac{n}{n+2} V_r \text{trace } J_Y(x_0) + V_rO(r^3)\right] \\
		&= r^2 \frac{n}{2(n+2)} \left(1 + O(r)\right)\left[\text{trace }J_Y(x_0) + O(r^3) \right]\;.
	\end{align}
	Dividing through by $r^2$ and taking the limit as $r\rightarrow 0$ proves the theorem.

\subsection*{Computation of Local Information}
	In this section, we provide a specification of the computational procedure used to estimate $J(X,Y) = \E_x[\text{trace } J_Y(X)]$ using blockgroup level data from the U.S. Census. For each fixed Census blockgroup $i$, let $P_i$ be the population, $A_i$ be the area, $\rho_i = P_i / A_i$ be the population density, and $p^i(y)$ be the observed proportion of racial group $y$. 

	Over the map we impose a hexagonal grid of radius $r$ (see \ref{fig:method} for an example). For each hex $k$ in the grid, let $N_k$ be the set of Census blockgroups overlapping $k$. Define $p^{k}(i) = \rho_i / \sum_{i \in N_k} \rho_i$ as the estimated proportion of population within hex $k$ residing in blockgroup $i$. Defining $p^k(i)$ this way reflects a simplifying assumption that each blockgroup in $N_k$ overlaps hex $k$ with equal area. Finally, define the estimated overall racial composition of hex $k$ through the formula $p^k(y) = \sum_{i \in N_k} p^{k}(i) p^i(y)$. The mutual information in hex $k$ is then: 
	\begin{equation}
		I(k) = \sum_{i \in N_k} p^k(i) D[p^i \| p^k]\;. 
	\end{equation}
	Using \eqref{eq:approx}, the estimated Fisher information at the centroid of hex $k$ is 
	\begin{equation}
		\text{trace } J_Y(k) \approx \frac{4 I(k)}{r^2} \triangleq J(k)\;.
	\end{equation}
	The estimated population in hex $k$ is $P_k = A^k\sum_{i \in N_k} \rho_i$, where $A^k$ is the cell area. We finally compute $\E_X[\text{trace} J_Y(X)]$ as 
	\begin{equation}
		J(X,Y) = \E_X[\text{trace }J_Y(X)] \approx \frac{1}{\sum_k P_k} \sum_k P_k J(k)
	\end{equation}

% \subsection*{Proof that $J(X,Y)$ Satisfies the Transfer Principle}

% 	As above, we consider the formula for $J(X,Y)$ in computational practice as 
% 	\begin{equation} 
% 		J(X,Y) \approx  \frac{1}{\sum_k P_k} \sum_k P_k J(k) = \frac{4}{r^2T} \sum_k P_k I(k), \label{comp}
% 	\end{equation}
% 	where we have defined $T$ as the total population, and where $k$ indexes the grid cells. 

% 	Suppose that we transfer an individual of race $y$ from blockgroup $i$ to blockgroup $j$ such that the following relation holds: 
% 	\begin{equation}
% 		p(y|X = i') \geq p(y|X = j')
% 	\end{equation}
% 	for all blockgroups $i'$ within radius $r$ of $i$ and all blockgroups $j'$ within radius $r$ of $j$. The transfer principle states that, in this case, $J'(X,Y) < J(X,Y)$, where $J'(X,Y)$ is the local information post-transfer. Using \eqref{comp}, it suffices to show that 

% 	\begin{equation}
% 		(P_k + 1) I'(k) + (P_\ell - 1) I'(\ell) < P_k I(k) + P_\ell I(\ell),
% 	\end{equation}
% 	where $I'(k)$ is the mutual information in hex $k$ post-transfer, for any grid cells $k$ overlapping $i$ and $\ell$ overlapping $j$. 

